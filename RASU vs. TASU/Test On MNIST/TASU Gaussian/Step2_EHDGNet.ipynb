{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cab34a5-470a-4ef9-a53b-ca4f162fbc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import PIL\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import random\n",
    "# from Modules import ConvBN, PoolConvBN, PoolLinearBN, SharpCosSim2d, SharpCosSimLinear, LReLU\n",
    "\n",
    "from ConvBN import ConvBN as ConvBN_BiasTrick\n",
    "from LinearBN import LinearBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f33668c-db76-4bee-b0c8-df89ea3975a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b89d4ea-df9a-44e0-a457-614c54abb89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)) # Normalize with mean 0.5 and std 0.5\n",
    "])\n",
    "\n",
    "batch_size= 1500\n",
    "num_workers=2\n",
    "pin_memory=True\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)\n",
    "\n",
    "dataset = torchvision.datasets.MNIST(root='../Data', train=True, download=True, transform=transform)\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [58000, 2000])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory, generator=g)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "test_set = torchvision.datasets.MNIST(root='../Data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b2e8b3d-3569-4db5-a7a4-66c193aa50ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0976ae0e-cf60-4c8b-a72b-9087c8a93353",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanH(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.alpha = nn.Parameter(torch.tensor(10.0)) \n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.tanh(self.alpha*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01280536-9fbc-4cad-98c4-700d8bdcc574",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.conv1_out = 32\n",
    "        self.conv1_size = 5\n",
    "        self.conv1_padding = 2\n",
    "\n",
    "\n",
    "        self.conv2_out = 64\n",
    "        self.conv2_size = 5\n",
    "        self.conv2_padding = 2\n",
    "\n",
    "        self.fc1_out = 512\n",
    "        self.fc2_out = 10\n",
    "\n",
    "        self.q = 1e-6\n",
    "        self.bias_trick_par = nn.Parameter(torch.tensor(0.00005))\n",
    "\n",
    "        # First Convolutional Block\n",
    "\n",
    "        self.block1 = ConvBN_BiasTrick(in_channels=1, out_channels=self.conv1_out, kernel_size=self.conv1_size, padding=self.conv1_padding, std = .05, bias_par_init=0.001)\n",
    "        self.block2 = ConvBN_BiasTrick(in_channels=self.conv1_out, out_channels=self.conv2_out, kernel_size=self.conv2_size, padding=self.conv2_padding, std = .05, bias_par_init=0.01)\n",
    "\n",
    "        # Second Convolutional Block\n",
    "       \n",
    "        self.block3 = LinearBN(in_features = self.conv2_out * (28//2//2) * (28//2//2), out_features=self.fc1_out, std=.3)\n",
    "        \n",
    "        \n",
    "        # torch.manual_seed(0)\n",
    "        self.w2 = nn.Parameter(torch.randn(self.fc1_out, self.fc2_out))\n",
    "        nn.init.normal_(self.w2, mean=0.0, std=.6)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.tanh = TanH()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.max_pool2d(self.tanh(self.block1(x)), (2,2), padding=0)\n",
    "        x = F.max_pool2d(self.tanh(self.block2(x)), (2,2), padding=0)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.tanh(self.block3(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = x + self.bias_trick_par\n",
    "        x_norm = x / (x.norm(p=2, dim=1, keepdim=True) + self.q)  # Normalize input x\n",
    "        w2_norm = self.w2 / (self.w2.norm(p=2, dim=1, keepdim=True) + self.q)  # Normalize weights\n",
    "        x = torch.matmul(x_norm, w2_norm) # Matrix multiplication \n",
    "\n",
    "        # Return raw logits (no softmax here, CrossEntropyLoss handles it)\n",
    "        return x\n",
    "\n",
    "    def custom_round(self, n):\n",
    "        remainder = n % 1000\n",
    "        base = n - remainder\n",
    "        if remainder >= 101:\n",
    "            return base + 1000\n",
    "        elif remainder <= 100:\n",
    "            return base\n",
    "            \n",
    "\n",
    "    def init_hdc(self, ratio, seed):\n",
    "        if not isinstance(ratio, (tuple, int)):\n",
    "            raise TypeError(\"ratio must be a tuple of size 4 or and integer\")\n",
    "\n",
    "        elif isinstance(ratio, (int)):\n",
    "            ratio = (ratio, ratio, ratio, ratio)\n",
    "            \n",
    "        if not isinstance(seed, (tuple)):\n",
    "            raise TypeError(\"seed must be a tuple of size 4\")\n",
    "        \n",
    "        self.block1.init_hdc(ratio = ratio[0], seed = seed[0])\n",
    "        self.block2.init_hdc(ratio = ratio[1], seed = seed[1])\n",
    "        self.block3.init_hdc(ratio = ratio[2], seed = seed[2])\n",
    "                \n",
    "        n_last = self.w2.size(0)\n",
    "        self.nHDC_last = int(self.custom_round(ratio[3] * n_last)) if ratio[3]<1000 else int(ratio[3])\n",
    "        torch.manual_seed(seed[3])\n",
    "        self.g = (torch.randn(self.w2.size(0), self.nHDC_last, device=self.w2.device)).to(torch.half)\n",
    "        self.wg = torch.sign(torch.matmul(self.g.t(), self.w2.to(torch.half)))\n",
    "\n",
    "    def hdc(self, x):\n",
    "        x = F.max_pool2d(torch.sign(self.block1.hdc(x)), (2,2), padding=0)\n",
    "        x = F.max_pool2d(torch.sign(self.block2.hdc(x)), (2,2), padding=0)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.sign(self.block3.hdc(x))\n",
    "\n",
    "        x = x + self.bias_trick_par\n",
    "        x = torch.sign(torch.matmul(x.to(torch.half), self.g))\n",
    "\n",
    "        return x\n",
    "        \n",
    "    def classification_layer(self, x):\n",
    "        x = x @ self.wg\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da8269bc-affa-4ce9-a47d-d8e3a14f786e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:20<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 1000, Block2: 1000, Block3: 1000 Classification Layer: 1000, Average Accuracy: 47.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:36<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 2000, Block2: 2000, Block3: 2000 Classification Layer: 2000, Average Accuracy: 74.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:53<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 3000, Block2: 3000, Block3: 3000 Classification Layer: 3000, Average Accuracy: 87.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:10<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 4000, Block2: 4000, Block3: 4000 Classification Layer: 4000, Average Accuracy: 92.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:26<00:00,  4.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 5000, Block2: 5000, Block3: 5000 Classification Layer: 5000, Average Accuracy: 94.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:43<00:00,  5.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 6000, Block2: 6000, Block3: 6000 Classification Layer: 6000, Average Accuracy: 95.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:00<00:00,  6.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 7000, Block2: 7000, Block3: 7000 Classification Layer: 7000, Average Accuracy: 96.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:16<00:00,  6.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 8000, Block2: 8000, Block3: 8000 Classification Layer: 8000, Average Accuracy: 96.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:34<00:00,  7.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 9000, Block2: 9000, Block3: 9000 Classification Layer: 9000, Average Accuracy: 97.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:50<00:00,  8.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 10000, Block2: 10000, Block3: 10000 Classification Layer: 10000, Average Accuracy: 97.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:06<00:00,  9.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 11000, Block2: 11000, Block3: 11000 Classification Layer: 11000, Average Accuracy: 97.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:23<00:00, 10.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 12000, Block2: 12000, Block3: 12000 Classification Layer: 12000, Average Accuracy: 97.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:40<00:00, 11.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 13000, Block2: 13000, Block3: 13000 Classification Layer: 13000, Average Accuracy: 97.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:57<00:00, 11.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 14000, Block2: 14000, Block3: 14000 Classification Layer: 14000, Average Accuracy: 97.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [04:14<00:00, 12.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 15000, Block2: 15000, Block3: 15000 Classification Layer: 15000, Average Accuracy: 97.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [04:30<00:00, 13.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 16000, Block2: 16000, Block3: 16000 Classification Layer: 16000, Average Accuracy: 98.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [04:47<00:00, 14.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 17000, Block2: 17000, Block3: 17000 Classification Layer: 17000, Average Accuracy: 98.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:03<00:00, 15.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 18000, Block2: 18000, Block3: 18000 Classification Layer: 18000, Average Accuracy: 98.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:19<00:00, 15.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 19000, Block2: 19000, Block3: 19000 Classification Layer: 19000, Average Accuracy: 98.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:37<00:00, 16.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 20000, Block2: 20000, Block3: 20000 Classification Layer: 20000, Average Accuracy: 98.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "from torch.nn.parallel import data_parallel\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model = Network().to(device)\n",
    "model.load_state_dict(torch.load('MNIST_GNet_Training_99.15.pth', weights_only = True))\n",
    "\n",
    "\n",
    "\n",
    "model.to(torch.half).to(device)\n",
    "model.eval()\n",
    "\n",
    "n_splits = 20\n",
    "split_size = len(test_set) // n_splits \n",
    "\n",
    "# scale = range\n",
    "scales = range(1000, 21000, 1000)\n",
    "accuracies = np.zeros((len(scales), n_splits))\n",
    "hyperdims = np.zeros((len(scales), 4))\n",
    "for i, scale in enumerate(scales):\n",
    "    indices = list(range(len(test_set)))\n",
    "    # np.random.seed(42)\n",
    "    # np.random.shuffle(indices)\n",
    "    for split_idx in tqdm(range(n_splits)):\n",
    "        start_idx = split_idx * split_size\n",
    "        end_idx = start_idx + split_size\n",
    "        split_indices = indices[start_idx:end_idx]\n",
    "        split_subset = Subset(test_set, split_indices)\n",
    "        split_loader = torch.utils.data.DataLoader(split_subset, batch_size=3, shuffle=False,\n",
    "                                                   num_workers=num_workers, pin_memory=pin_memory)\n",
    "        # ratio = (12, 1.15/6, 3, 18)\n",
    "        # ratio = tuple(scale * r for r in ratio)\n",
    "        ratio = scale\n",
    "        torch.manual_seed(split_idx+4)\n",
    "        random_seeds = tuple(torch.randint(0, 1000, (1,)).item() for _ in range(4))\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        model.init_hdc(ratio, random_seeds)\n",
    "        hyperdims[i] = np.array([model.block1.nHDC, model.block2.nHDC, model.block3.nHDC, model.nHDC_last])\n",
    "        correct = 0\n",
    "        total = 0\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for images, labels in (split_loader):\n",
    "                images, labels = images.cuda(non_blocking=True), labels.cuda(non_blocking=True)\n",
    "                output = model.hdc(images.to(torch.half))\n",
    "                output = model.classification_layer(output.to(torch.half))\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "    \n",
    "        acc = 100 * correct / total\n",
    "    \n",
    "        accuracies[i, split_idx] = acc\n",
    "    \n",
    "    print(f'Block1: {model.block1.nHDC}, Block2: {model.block2.nHDC}, Block3: {model.block3.nHDC} Classification Layer: {model.nHDC_last}, Average Accuracy: {np.mean(accuracies[i]):.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5360de05-be62-4ecc-8a51-f5cf70550bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47.26 74.18 87.85 92.35 94.22 95.92 96.72 96.91 97.42 97.47 97.59 97.86\n",
      " 97.82 97.94 97.93 98.29 98.12 98.19 98.21 98.33]\n",
      "[ 1000.  2000.  3000.  4000.  5000.  6000.  7000.  8000.  9000. 10000.\n",
      " 11000. 12000. 13000. 14000. 15000. 16000. 17000. 18000. 19000. 20000.]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(accuracies, axis=1))\n",
    "print(np.mean(hyperdims, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cac83af-59ad-4576-bf3f-459d14b9fb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat\n",
    "savemat('HDCGNet_MNIST.mat', {'HDCGNet_MNIST': accuracies})\n",
    "savemat('HDCGNet_MNIST_nHDC.mat', {'HDCGNet_MNIST_nHDC': hyperdims})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53aae01-eb0e-4840-8831-d28f36f21056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
