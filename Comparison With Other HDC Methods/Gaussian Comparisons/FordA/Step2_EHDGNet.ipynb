{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eb42aeb-5d9d-4852-b5f9-41e2bbaa8071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from ConvBN1d import ConvBN\n",
    "from LinearBN import LinearBN\n",
    "from source import train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f0f73ef-c40f-4c66-88e8-d79ba92e1722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (3601, 500), Test shape: (1320, 500)\n"
     ]
    }
   ],
   "source": [
    "def load_ucr(file):\n",
    "    data = np.loadtxt(file)\n",
    "    X = data[:, 1:]\n",
    "    y = data[:, 0]\n",
    "    y = np.where(y == 1, 1, 0)  # convert labels to 0/1\n",
    "    return X, y\n",
    "\n",
    "# Adjust file paths to your local files\n",
    "X_train, y_train = load_ucr(\"FordA_TRAIN.txt\")\n",
    "X_test, y_test = load_ucr(\"FordA_TEST.txt\")\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90e19080-8e31-4cd1-a971-148fa5b061eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c8b0480-0365-4a55-ba08-275a4cde5fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = X_train_tensor.mean()\n",
    "std = X_train_tensor.std()\n",
    "X_train_tensor = (X_train_tensor - mean) / std\n",
    "X_test_tensor = (X_test_tensor - mean) / std\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f57619b-c5d6-45a7-b655-154f25f3e12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LReLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LReLU, self).__init__()\n",
    "        self.alpha = nn.Parameter(torch.tensor(5.0)) \n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.relu(self.alpha*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c6604d6-f1f9-4f5a-93db-1911a80137da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.conv1_out = 16\n",
    "        self.conv1_size = 15\n",
    "        self.conv1_padding = 7\n",
    "\n",
    "\n",
    "        self.conv2_out = 16\n",
    "        self.conv2_size = 15\n",
    "        self.conv2_padding = 7\n",
    "\n",
    "        self.conv3_out = 25\n",
    "        self.conv3_size = 13\n",
    "        self.conv3_padding = 6\n",
    "\n",
    "        self.fc1_out = 2\n",
    "\n",
    "        self.q = 1e-6\n",
    "        self.bias_trick_par = nn.Parameter(torch.tensor(0.00005))\n",
    "\n",
    "        # First Convolutional Block\n",
    "\n",
    "        self.block1 = ConvBN(in_channels=1, out_channels=self.conv1_out, kernel_size=self.conv1_size, padding=self.conv1_padding, std = .05, bias_par_init=0.0015)\n",
    "        self.block2 = ConvBN(in_channels=self.conv1_out, out_channels=self.conv2_out, kernel_size=self.conv2_size, padding=self.conv2_padding, std = .15, bias_par_init=0.0015)\n",
    "        self.block3 = ConvBN(in_channels=self.conv2_out, out_channels=self.conv3_out, kernel_size=self.conv3_size, padding=self.conv3_padding, std = .15, bias_par_init=0.0015)\n",
    "               \n",
    "        \n",
    "        # torch.manual_seed(0)\n",
    "        self.w2 = nn.Parameter(torch.randn(self.conv3_out * 500, self.fc1_out))\n",
    "        nn.init.normal_(self.w2, mean=0.0, std=.6)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.relu = LReLU()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.relu(self.block1(x))\n",
    "        x = self.relu(self.block2(x))\n",
    "        x = self.relu(self.block3(x))\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # x = self.relu(self.block3(x))\n",
    "        # x = self.dropout(x)\n",
    "\n",
    "        x = x + self.bias_trick_par\n",
    "        x_norm = x / (x.norm(p=2, dim=1, keepdim=True) + self.q)  # Normalize input x\n",
    "        w2_norm = self.w2 / (self.w2.norm(p=2, dim=1, keepdim=True) + self.q)  # Normalize weights\n",
    "        x = torch.matmul(x_norm, w2_norm) # Matrix multiplication \n",
    "\n",
    "        # Return raw logits (no softmax here, CrossEntropyLoss handles it)\n",
    "        return x\n",
    "\n",
    "    def custom_round(self, n):\n",
    "        remainder = n % 1000\n",
    "        base = n - remainder\n",
    "        if remainder >= 101:\n",
    "            return base + 1000\n",
    "        elif remainder <= 100:\n",
    "            return base\n",
    "\n",
    "    def init_hdc(self, ratio, seed):\n",
    "        if not isinstance(ratio, (tuple, int)):\n",
    "            raise TypeError(\"ratio must be a tuple of size 4 or and integer\")\n",
    "\n",
    "        elif isinstance(ratio, (int)):\n",
    "            ratio = (ratio, ratio, ratio, ratio)\n",
    "            \n",
    "        if not isinstance(seed, (tuple)):\n",
    "            raise TypeError(\"seed must be a tuple of size 4\")\n",
    "        \n",
    "        self.block1.init_hdc(ratio = ratio[0], seed = seed[0])\n",
    "        self.block2.init_hdc(ratio = ratio[1], seed = seed[1])\n",
    "        self.block3.init_hdc(ratio = ratio[2], seed = seed[2])\n",
    "                \n",
    "        self.n_last = self.w2.size(0)\n",
    "        self.nHDC_last = int(self.custom_round(ratio[3] * self.n_last)) if ratio[3]<1000 else int(ratio[3])\n",
    "        torch.manual_seed(seed[3])\n",
    "        self.g = (torch.randn(self.w2.size(0), self.nHDC_last, device=self.w2.device)).to(torch.half)\n",
    "        self.wg = torch.sign(torch.matmul(self.g.t(), self.w2.to(torch.half)))\n",
    "\n",
    "\n",
    "    def hdc(self, x):\n",
    "        x = self.relu(self.block1.hdc(x))\n",
    "        x = self.relu(self.block2.hdc(x))\n",
    "        x = self.relu(self.block3.hdc(x))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = x + self.bias_trick_par\n",
    "        x = torch.sign(torch.matmul(x.to(torch.half), self.g))\n",
    "\n",
    "        return x\n",
    "        \n",
    "    def classification_layer(self, x):\n",
    "        x = x @ self.wg\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "696512f5-24b0-4b98-8e3e-1165b30dd300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "from torch.utils.data import Subset\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model = Network().to(device)\n",
    "model.load_state_dict(torch.load('FordA_GNet_Training_92.35.pth', weights_only = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df8e6271-6353-47b1-a473-7c2b94dfa8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132\n",
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:08<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 5000, Block2: 5000, Block3: 5000, Classification Layer: 5000, Avg Acc: 81.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:09<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 6000, Block2: 6000, Block3: 6000, Classification Layer: 6000, Avg Acc: 81.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:11<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 7000, Block2: 7000, Block3: 7000, Classification Layer: 7000, Avg Acc: 82.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:13<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 8000, Block2: 8000, Block3: 8000, Classification Layer: 8000, Avg Acc: 84.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 9000, Block2: 9000, Block3: 9000, Classification Layer: 9000, Avg Acc: 83.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:16<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 10000, Block2: 10000, Block3: 10000, Classification Layer: 10000, Avg Acc: 85.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:18<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 11000, Block2: 11000, Block3: 11000, Classification Layer: 11000, Avg Acc: 85.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:21<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 12000, Block2: 12000, Block3: 12000, Classification Layer: 12000, Avg Acc: 85.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:22<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 13000, Block2: 13000, Block3: 13000, Classification Layer: 13000, Avg Acc: 86.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:24<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 14000, Block2: 14000, Block3: 14000, Classification Layer: 14000, Avg Acc: 87.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:26<00:00,  2.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 15000, Block2: 15000, Block3: 15000, Classification Layer: 15000, Avg Acc: 87.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:28<00:00,  2.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 16000, Block2: 16000, Block3: 16000, Classification Layer: 16000, Avg Acc: 88.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:30<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 17000, Block2: 17000, Block3: 17000, Classification Layer: 17000, Avg Acc: 89.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:31<00:00,  3.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 18000, Block2: 18000, Block3: 18000, Classification Layer: 18000, Avg Acc: 88.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:33<00:00,  3.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 19000, Block2: 19000, Block3: 19000, Classification Layer: 19000, Avg Acc: 88.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:35<00:00,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1: 20000, Block2: 20000, Block3: 20000, Classification Layer: 20000, Avg Acc: 89.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(torch.half).to(device)\n",
    "model.eval()\n",
    "n_splits = 10\n",
    "split_size = len(test_dataset) // n_splits  # 10000 // 20 = 500\n",
    "print(split_size)\n",
    "\n",
    "times = []\n",
    "num_workers=2\n",
    "pin_memory=True\n",
    "hyperdims = range(5000, 21000, 1000)\n",
    "print(len(hyperdims))\n",
    "NHDC = np.zeros((len(hyperdims), 4))\n",
    "accuracies = np.zeros((len(hyperdims), n_splits))\n",
    "for i, nhdc in enumerate(hyperdims):\n",
    "    indices = list(range(len(test_dataset)))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(indices)  # or random.shuffle(indices)\n",
    "    for split_idx in tqdm(range(n_splits)):\n",
    "        start_idx = split_idx * split_size\n",
    "        end_idx = start_idx + split_size\n",
    "        split_indices = indices[start_idx:end_idx]\n",
    "        split_subset = Subset(test_dataset, split_indices)\n",
    "        split_loader = torch.utils.data.DataLoader(split_subset, batch_size=15, shuffle=False,\n",
    "                                                   num_workers=num_workers, pin_memory=pin_memory)\n",
    "        torch.manual_seed(split_idx)\n",
    "        random_seeds = tuple(torch.randint(0, 1000, (1,)).item() for _ in range(4))\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        model.init_hdc(nhdc, random_seeds)\n",
    "        NHDC[i] = [model.block1.nHDC, model.block2.nHDC, model.block3.nHDC, model.nHDC_last]\n",
    "        correct = 0\n",
    "        total = 0\n",
    "    \n",
    "        t0 = time.time()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in (split_loader):\n",
    "                images, labels = images.cuda(non_blocking=True), labels.cuda(non_blocking=True)\n",
    "                output = model.hdc(images.to(torch.half))\n",
    "                output = model.classification_layer(output.to(torch.half))\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        t1 = time.time()\n",
    "    \n",
    "        acc = 100 * correct / total\n",
    "        dt = t1 - t0\n",
    "    \n",
    "        accuracies[i, split_idx] = acc\n",
    "        times.append(dt)\n",
    "    \n",
    "    print(f'Block1: {model.block1.nHDC}, Block2: {model.block2.nHDC}, Block3: {model.block3.nHDC}, Classification Layer: {model.nHDC_last}, Avg Acc: {np.mean(accuracies[i]):.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b00d036d-9a52-4b6c-905b-ed5426811fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([81.51515152, 81.66666667, 82.04545455, 84.16666667, 83.93939394,\n",
       "        85.22727273, 85.45454545, 85.90909091, 86.21212121, 87.27272727,\n",
       "        87.27272727, 88.10606061, 89.62121212, 88.63636364, 88.86363636,\n",
       "        89.24242424]),\n",
       " array([4.39393939, 3.26373625, 3.92260408, 4.35787182, 2.11036186,\n",
       "        4.30220329, 5.58554806, 2.85878216, 3.35050673, 2.87080232,\n",
       "        3.06420431, 1.8572198 , 2.97390703, 2.34726263, 1.62658413,\n",
       "        2.76904044]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(accuracies, axis=1), np.std(accuracies, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03220a94-07b4-43c2-b2b7-c6e873707b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat\n",
    "savemat('FordA_EHDGNet.mat',{'FordA_EHDGNet':accuracies})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5088e104-438b-4046-9b9b-c696b8262444",
   "metadata": {},
   "outputs": [],
   "source": [
    "savemat('FordA_nHD.mat', {'FordA_nHD':NHDC})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4d0b1b-e49c-4a16-a87b-deb147c635d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
