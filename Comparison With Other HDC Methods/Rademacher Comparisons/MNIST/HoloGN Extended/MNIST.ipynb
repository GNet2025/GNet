{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09bba484-669d-4f7d-8876-f272fcf9addc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "# import torch\n",
    "# from torchvision.datasets import FashionMNIST, MNIST\n",
    "# from torchvision.transforms import ToTensor, Lambda\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import gzip\n",
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import Tensor\n",
    "from torchvision import datasets, transforms\n",
    "from scipy.io import loadmat, savemat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a63fe9f4-eba1-4c7f-b7e2-131a31dbec40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5000,  6000,  7000,  8000,  9000, 10000, 11000, 11750, 12750,\n",
       "       13750, 14750])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperdims = loadmat('../EHDGNet_MNIST_nHD.mat')\n",
    "hyperdims = hyperdims['EHDGNet_MNIST_nHD']\n",
    "hyperdims = np.mean(hyperdims, axis=1, dtype=int)\n",
    "hyperdims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3e423a0-bdf6-43fe-a882-8f8d3bc2391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Device and hyperparameters\n",
    "device     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_splits   = 20\n",
    "hyperdims  = hyperdims\n",
    "batch_size = 10\n",
    "\n",
    "# 2. Load & preprocess MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                           # → [0,1], shape (1,28,28)\n",
    "    transforms.Lambda(lambda x: (x > 0.5).float()),  # binarize\n",
    "    transforms.Lambda(lambda x: x.view(-1))         # flatten → (784,)\n",
    "])\n",
    "\n",
    "train_ds = datasets.MNIST(root='../../Data', train=True,  download=True, transform=transform)\n",
    "test_ds  = datasets.MNIST(root='../../Data', train=False, download=True, transform=transform)\n",
    "\n",
    "X_train = torch.stack([img for img, _ in train_ds], dim=0).to(device)  # (60000, 784)\n",
    "y_train = torch.tensor([lbl for _, lbl in train_ds], device=device)\n",
    "\n",
    "X_test  = torch.stack([img for img, _ in test_ds],  dim=0).to(device)  # (10000, 784)\n",
    "y_test  = torch.tensor([lbl for _, lbl in test_ds],  device=device)\n",
    "\n",
    "B = X_train.size(1)                   # 784 pixels\n",
    "C = len(torch.unique(y_train))        # 10 classes\n",
    "split_size = X_test.size(0) // n_splits\n",
    "\n",
    "# 3. HDC utility functions\n",
    "\n",
    "def generate_base_HDVs(D, B, device):\n",
    "    \"\"\"Generate a (B × D) random binary matrix.\"\"\"\n",
    "    return (torch.rand(B, D, device=device) > 0.5).int()  # intTensor of 0/1\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_dataset_batched(X, base_HDVs, batch_size=128):\n",
    "    \"\"\"\n",
    "    Encode X in chunks to avoid OOM.\n",
    "    X:          (N, B) floatTensor {0,1}\n",
    "    base_HDVs:  (B, D) intTensor {0,1}\n",
    "    returns:    (N, D) intTensor {0,1}\n",
    "    \"\"\"\n",
    "    N, B = X.shape\n",
    "    D    = base_HDVs.shape[1]\n",
    "\n",
    "    # Precompute roll-shifted HDVs once\n",
    "    perm_HDVs = base_HDVs.roll(shifts=1, dims=1)  # (B, D)\n",
    "\n",
    "    # Expand for broadcasting\n",
    "    base = base_HDVs.unsqueeze(0)   # (1, B, D)\n",
    "    perm = perm_HDVs.unsqueeze(0)   # (1, B, D)\n",
    "\n",
    "    chunks = []\n",
    "    for i in (range(0, N, batch_size)):\n",
    "        xb    = X[i : i+batch_size]           # (b, B)\n",
    "        xb_exp= xb.unsqueeze(-1)              # (b, B, 1)\n",
    "\n",
    "        # When pixel==1 pick perm, else pick base\n",
    "        weighted = xb_exp * perm + (1 - xb_exp) * base  # (b, B, D)\n",
    "        H_float  = weighted.mean(dim=1)                 # (b, D)\n",
    "        chunks.append(torch.round(H_float).int())       # (b, D)\n",
    "\n",
    "    return torch.cat(chunks, dim=0)  # (N, D)\n",
    "\n",
    "def encode_class_HDVs(H_train, y_train, C):\n",
    "    \"\"\"\n",
    "    Bundle all train-HDVs per class.\n",
    "    H_train: (N, D), y_train: (N,)\n",
    "    returns: (C, D)\n",
    "    \"\"\"\n",
    "    class_HDVs = []\n",
    "    for c in range(C):\n",
    "        subset = H_train[y_train == c]        # (Nc, D)\n",
    "        m      = subset.float().mean(dim=0)   # (D,)\n",
    "        class_HDVs.append(torch.round(m).int())\n",
    "    return torch.stack(class_HDVs, dim=0)    # (C, D)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(H_test, class_HDVs):\n",
    "    \"\"\"\n",
    "    Nearest-neighbor by Hamming distance.\n",
    "    H_test:     (M, D), class_HDVs: (C, D)\n",
    "    returns:    (M,) predicted labels\n",
    "    \"\"\"\n",
    "    diffs = H_test.unsqueeze(1) != class_HDVs.unsqueeze(0)  # (M, C, D)\n",
    "    dists = diffs.sum(dim=2)                                # (M, C)\n",
    "    return dists.argmin(dim=1)                              # (M,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ecad34f-a084-4791-8cb8-50bd89bf91d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> Hyperdimension: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 12.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 0.7948 for Hyperdim 5000\n",
      "\n",
      "==> Hyperdimension: 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 10.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 0.7973 for Hyperdim 6000\n",
      "\n",
      "==> Hyperdimension: 7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 0.8066 for Hyperdim 7000\n",
      "\n",
      "==> Hyperdimension: 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  8.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 0.7953 for Hyperdim 8000\n",
      "\n",
      "==> Hyperdimension: 9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  7.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 0.8066 for Hyperdim 9000\n",
      "\n",
      "==> Hyperdimension: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:03<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 0.8093 for Hyperdim 10000\n",
      "\n",
      "==> Hyperdimension: 11000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:03<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 0.8044 for Hyperdim 11000\n",
      "\n",
      "==> Hyperdimension: 11750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:03<00:00,  5.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 0.8059 for Hyperdim 11750\n",
      "\n",
      "==> Hyperdimension: 12750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:03<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 0.8104 for Hyperdim 12750\n",
      "\n",
      "==> Hyperdimension: 13750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:04<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 0.8095 for Hyperdim 13750\n",
      "\n",
      "==> Hyperdimension: 14750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:04<00:00,  4.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 0.8036 for Hyperdim 14750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_splits   = 20\n",
    "hyperdims  = hyperdims\n",
    "accuracies = np.zeros((len(hyperdims), n_splits), dtype=float)\n",
    "for idx, D in enumerate(hyperdims):\n",
    "    print(f\"\\n==> Hyperdimension: {D}\")\n",
    "    base_HDVs  = generate_base_HDVs(D, B, device)\n",
    "    H_train    = encode_dataset_batched(X_train, base_HDVs, batch_size)\n",
    "    class_HDVs = encode_class_HDVs(H_train, y_train, C)\n",
    "\n",
    "    for i in tqdm(range(n_splits)):\n",
    "        s, e = i * split_size, (i + 1) * split_size\n",
    "        Xs, ys = X_test[s:e], y_test[s:e]\n",
    "\n",
    "        Hs    = encode_dataset_batched(Xs, base_HDVs, batch_size)\n",
    "        preds = predict(Hs, class_HDVs)\n",
    "    \n",
    "        accuracies[idx, i] = (preds == ys).float().mean().item()\n",
    "        # print(f'Accuracy for split index {i}: {accuracies[idx, i]}')\n",
    "\n",
    "    print(f\"Average accuracy: {accuracies[idx].mean().item():.4f} for Hyperdim {D}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "719833bb-f58b-47c9-a9b8-df2157a414ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat, loadmat\n",
    "savemat('HoloGN_MNIST.mat', {'HoloGN_MNIST': accuracies*100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324d1f32-7bc9-4212-b389-57f6a5067adf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
