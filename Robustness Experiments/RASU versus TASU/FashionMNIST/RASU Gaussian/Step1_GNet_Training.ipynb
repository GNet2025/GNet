{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85fdf28c-f372-4dd7-bd54-a7aa2ed65849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import PIL\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "# from Modules import ConvBN, PoolConvBN, PoolLinearBN, SharpCosSim2d, SharpCosSimLinear, LReLU\n",
    "\n",
    "from ConvBN import ConvBN as ConvBN_BiasTrick\n",
    "from LinearBN import LinearBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "748ecd3a-7ffc-4bbb-8694-a567b6e10082",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LReLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LReLU, self).__init__()\n",
    "        self.alpha = nn.Parameter(torch.tensor(5.0)) \n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.relu(self.alpha*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cff8a703-c4d7-4c63-98fc-97d8ef46a5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)) # Normalize with mean 0.5 and std 0.5\n",
    "])\n",
    "\n",
    "batch_size= 2000\n",
    "num_workers=2\n",
    "pin_memory=True\n",
    "\n",
    "dataset = torchvision.datasets.FashionMNIST(root='../', train=True, download=True, transform=transform)\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [58000, 2000])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "test_set = torchvision.datasets.FashionMNIST(root='../', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aced529a-d3ba-4a60-980e-2623e755c88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7565906-2310-4a40-b7a7-627f2e2de45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.conv1_out = 32\n",
    "        self.conv1_size = 5\n",
    "        self.conv1_padding = 2\n",
    "\n",
    "\n",
    "        # self.conv2_out = 16\n",
    "        # self.conv2_size = 5\n",
    "        # self.conv2_padding = 0\n",
    "\n",
    "        self.fc1_out = 512\n",
    "        self.fc2_out = 10\n",
    "\n",
    "        self.q = 1e-6\n",
    "        self.bias_trick_par = torch.nn.Parameter(torch.tensor(0.00005))\n",
    "\n",
    "        # First Convolutional Block\n",
    "\n",
    "        self.block1 = ConvBN_BiasTrick(in_channels=1, out_channels=self.conv1_out, kernel_size=self.conv1_size, padding=self.conv1_padding, std = .1)\n",
    "\n",
    "        size = 28 # 28 - 5 + 4 = 25 + 1 = 26\n",
    "        self.psize = 3\n",
    "        # Second Convolutional Block\n",
    "       \n",
    "        self.block3 = LinearBN(in_features = self.conv1_out * (size//self.psize) * (size//self.psize), out_features=self.fc1_out, std=.1)\n",
    "        \n",
    "        \n",
    "        self.w2 = nn.Parameter(torch.randn(self.fc1_out, self.fc2_out))\n",
    "        nn.init.normal_(self.w2, mean=0.0, std=.1)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.dropout2d = nn.Dropout2d(0.25)\n",
    "\n",
    "        self.relu = LReLU()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.max_pool2d(self.relu(self.block1(x)), (self.psize,self.psize), padding=0)\n",
    "        x = self.dropout2d(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.relu(self.block3(x))\n",
    "        # x = self.dropout(x)\n",
    "\n",
    "        x = x + self.bias_trick_par\n",
    "        x_norm = x / (x.norm(p=2, dim=1, keepdim=True) + self.q)  # Normalize input x\n",
    "        w2_norm = self.w2 / (self.w2.norm(p=2, dim=1, keepdim=True) + self.q)  # Normalize weights\n",
    "        x = torch.matmul(x_norm, w2_norm) # Matrix multiplication \n",
    "\n",
    "        # Return raw logits (no softmax here, CrossEntropyLoss handles it)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fab82256-86aa-432f-a31d-a6f210a08745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] loss: 0.00057 accuracy: 0.6704 val loss: 0.00031 val accuracy: 0.7890 time: 5.51 seconds\n",
      "[epoch 2] loss: 0.00029 accuracy: 0.8068 val loss: 0.00024 val accuracy: 0.8410 time: 4.79 seconds\n",
      "[epoch 3] loss: 0.00024 accuracy: 0.8474 val loss: 0.00021 val accuracy: 0.8630 time: 4.71 seconds\n",
      "[epoch 4] loss: 0.00021 accuracy: 0.8634 val loss: 0.00019 val accuracy: 0.8750 time: 4.78 seconds\n",
      "[epoch 5] loss: 0.00019 accuracy: 0.8745 val loss: 0.00018 val accuracy: 0.8800 time: 4.79 seconds\n",
      "[epoch 6] loss: 0.00018 accuracy: 0.8811 val loss: 0.00017 val accuracy: 0.8805 time: 4.81 seconds\n",
      "[epoch 7] loss: 0.00017 accuracy: 0.8883 val loss: 0.00016 val accuracy: 0.8910 time: 4.75 seconds\n",
      "[epoch 8] loss: 0.00016 accuracy: 0.8934 val loss: 0.00016 val accuracy: 0.8940 time: 4.78 seconds\n",
      "[epoch 9] loss: 0.00016 accuracy: 0.8970 val loss: 0.00015 val accuracy: 0.8955 time: 4.72 seconds\n",
      "[epoch 10] loss: 0.00015 accuracy: 0.9009 val loss: 0.00015 val accuracy: 0.8950 time: 4.75 seconds\n",
      "[epoch 11] loss: 0.00014 accuracy: 0.9031 val loss: 0.00014 val accuracy: 0.9010 time: 4.80 seconds\n",
      "[epoch 12] loss: 0.00014 accuracy: 0.9067 val loss: 0.00014 val accuracy: 0.9045 time: 4.73 seconds\n",
      "[epoch 13] loss: 0.00014 accuracy: 0.9089 val loss: 0.00014 val accuracy: 0.9065 time: 4.82 seconds\n",
      "[epoch 14] loss: 0.00013 accuracy: 0.9112 val loss: 0.00014 val accuracy: 0.9070 time: 4.77 seconds\n",
      "[epoch 15] loss: 0.00013 accuracy: 0.9147 val loss: 0.00014 val accuracy: 0.9040 time: 4.71 seconds\n",
      "[epoch 16] loss: 0.00013 accuracy: 0.9165 val loss: 0.00013 val accuracy: 0.9080 time: 4.69 seconds\n",
      "[epoch 17] loss: 0.00012 accuracy: 0.9181 val loss: 0.00013 val accuracy: 0.9105 time: 4.85 seconds\n",
      "[epoch 18] loss: 0.00012 accuracy: 0.9209 val loss: 0.00013 val accuracy: 0.9120 time: 4.73 seconds\n",
      "[epoch 19] loss: 0.00012 accuracy: 0.9218 val loss: 0.00013 val accuracy: 0.9130 time: 4.75 seconds\n",
      "[epoch 20] loss: 0.00012 accuracy: 0.9234 val loss: 0.00013 val accuracy: 0.9150 time: 4.88 seconds\n",
      "[epoch 21] loss: 0.00011 accuracy: 0.9254 val loss: 0.00012 val accuracy: 0.9145 time: 4.74 seconds\n",
      "[epoch 22] loss: 0.00011 accuracy: 0.9267 val loss: 0.00012 val accuracy: 0.9145 time: 4.83 seconds\n",
      "[epoch 23] loss: 0.00011 accuracy: 0.9275 val loss: 0.00013 val accuracy: 0.9110 time: 4.69 seconds\n",
      "[epoch 24] loss: 0.00011 accuracy: 0.9293 val loss: 0.00012 val accuracy: 0.9140 time: 4.82 seconds\n",
      "[epoch 25] loss: 0.00011 accuracy: 0.9317 val loss: 0.00012 val accuracy: 0.9175 time: 4.71 seconds\n",
      "[epoch 26] loss: 0.00011 accuracy: 0.9313 val loss: 0.00012 val accuracy: 0.9160 time: 4.72 seconds\n",
      "[epoch 27] loss: 0.00010 accuracy: 0.9343 val loss: 0.00012 val accuracy: 0.9205 time: 4.74 seconds\n",
      "[epoch 28] loss: 0.00010 accuracy: 0.9349 val loss: 0.00012 val accuracy: 0.9200 time: 4.71 seconds\n",
      "[epoch 29] loss: 0.00010 accuracy: 0.9352 val loss: 0.00012 val accuracy: 0.9200 time: 4.70 seconds\n",
      "[epoch 30] loss: 0.00010 accuracy: 0.9358 val loss: 0.00012 val accuracy: 0.9240 time: 4.69 seconds\n",
      "[epoch 31] loss: 0.00010 accuracy: 0.9367 val loss: 0.00012 val accuracy: 0.9185 time: 4.79 seconds\n",
      "[epoch 32] loss: 0.00010 accuracy: 0.9390 val loss: 0.00012 val accuracy: 0.9215 time: 4.68 seconds\n",
      "[epoch 33] loss: 0.00010 accuracy: 0.9399 val loss: 0.00012 val accuracy: 0.9215 time: 4.95 seconds\n",
      "[epoch 34] loss: 0.00009 accuracy: 0.9412 val loss: 0.00012 val accuracy: 0.9220 time: 4.72 seconds\n",
      "[epoch 35] loss: 0.00009 accuracy: 0.9426 val loss: 0.00012 val accuracy: 0.9195 time: 4.82 seconds\n",
      "[epoch 36] loss: 0.00009 accuracy: 0.9429 val loss: 0.00012 val accuracy: 0.9190 time: 4.74 seconds\n",
      "[epoch 37] loss: 0.00009 accuracy: 0.9448 val loss: 0.00012 val accuracy: 0.9220 time: 4.81 seconds\n",
      "[epoch 38] loss: 0.00009 accuracy: 0.9451 val loss: 0.00012 val accuracy: 0.9190 time: 4.78 seconds\n",
      "[epoch 39] loss: 0.00009 accuracy: 0.9446 val loss: 0.00011 val accuracy: 0.9235 time: 4.80 seconds\n",
      "[epoch 40] loss: 0.00009 accuracy: 0.9467 val loss: 0.00012 val accuracy: 0.9205 time: 4.81 seconds\n",
      "[epoch 41] loss: 0.00009 accuracy: 0.9478 val loss: 0.00011 val accuracy: 0.9275 time: 4.76 seconds\n",
      "[epoch 42] loss: 0.00009 accuracy: 0.9481 val loss: 0.00012 val accuracy: 0.9225 time: 4.76 seconds\n",
      "[epoch 43] loss: 0.00009 accuracy: 0.9485 val loss: 0.00011 val accuracy: 0.9210 time: 4.73 seconds\n",
      "[epoch 44] loss: 0.00008 accuracy: 0.9506 val loss: 0.00011 val accuracy: 0.9235 time: 4.75 seconds\n",
      "[epoch 45] loss: 0.00008 accuracy: 0.9507 val loss: 0.00012 val accuracy: 0.9240 time: 4.76 seconds\n",
      "[epoch 46] loss: 0.00008 accuracy: 0.9522 val loss: 0.00012 val accuracy: 0.9210 time: 4.67 seconds\n",
      "[epoch 47] loss: 0.00008 accuracy: 0.9529 val loss: 0.00011 val accuracy: 0.9210 time: 4.75 seconds\n",
      "[epoch 48] loss: 0.00008 accuracy: 0.9544 val loss: 0.00011 val accuracy: 0.9255 time: 4.67 seconds\n",
      "[epoch 49] loss: 0.00008 accuracy: 0.9549 val loss: 0.00011 val accuracy: 0.9245 time: 4.69 seconds\n",
      "[epoch 50] loss: 0.00008 accuracy: 0.9552 val loss: 0.00011 val accuracy: 0.9225 time: 4.81 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import time  # Import time module\n",
    "\n",
    "train = True\n",
    "model = Network().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Trained 100 Epochs with lr=0.025, 50 epochs with 0.005 and 50 epochs with 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.00001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "if train:\n",
    "    \n",
    "    loss_hist, acc_hist = [], []\n",
    "    loss_hist_val, acc_hist_val = [], []\n",
    "    \n",
    "    # Initialize variable to track the lowest validation accuracy\n",
    "    best_val_acc = -float('inf')  # Set to negative infinity initially to track the maximum accuracy\n",
    "    \n",
    "    for epoch in range(50):\n",
    "        start_time = time.time()  # Record the start time of the epoch\n",
    "    \n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        for data in train_loader:\n",
    "            batch, labels = data\n",
    "            batch, labels = batch.to(device), labels.to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            # Compute training statistics\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            running_loss += loss.item()\n",
    "    \n",
    "        avg_loss = running_loss / len(train_set)\n",
    "        avg_acc = correct / len(train_set)\n",
    "        loss_hist.append(avg_loss)\n",
    "        acc_hist.append(avg_acc)\n",
    "    \n",
    "        # Validation statistics\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss_val = 0.0\n",
    "            correct_val = 0\n",
    "            for data in val_loader:\n",
    "                batch, labels = data\n",
    "                batch, labels = batch.to(device), labels.to(device)\n",
    "                outputs = model(batch)\n",
    "                loss = criterion(outputs, labels)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "                loss_val += loss.item()\n",
    "            avg_loss_val = loss_val / len(val_set)\n",
    "            avg_acc_val = correct_val / len(val_set)\n",
    "            loss_hist_val.append(avg_loss_val)\n",
    "            acc_hist_val.append(avg_acc_val)\n",
    "        model.train()\n",
    "    \n",
    "        scheduler.step(avg_loss_val)\n",
    "    \n",
    "        # Check if the current validation accuracy is the best we've seen\n",
    "        if avg_acc_val > best_val_acc:\n",
    "            best_val_acc = avg_acc_val\n",
    "            # Save the model with the highest validation accuracy\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                torch.save(model.module.state_dict(), 'best_model_fashionmnist.pt')\n",
    "            else:\n",
    "                torch.save(model.state_dict(), 'best_model_fashionmnist.pt')\n",
    "    \n",
    "        # Calculate elapsed time\n",
    "        elapsed_time = time.time() - start_time  # Calculate the time taken for this epoch\n",
    "    \n",
    "        print('[epoch %d] loss: %.5f accuracy: %.4f val loss: %.5f val accuracy: %.4f time: %.2f seconds' %\n",
    "              (epoch + 1, avg_loss, avg_acc, avg_loss_val, avg_acc_val, elapsed_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a83be87-bf65-4a82-95b6-89acf3fb356d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.00012, Test accuracy: 0.9186\n"
     ]
    }
   ],
   "source": [
    "# Load the best model saved during training\n",
    "# Trained 100 Epochs with lr=0.025, 50 epochs with 0.005 and 50 epochs with 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.00001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "test_loss = 0.0\n",
    "correct_test = 0\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=2000, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "\n",
    "# Evaluate on the test dataset\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # Get predictions and update the correct count\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "# Compute average loss and accuracy for the test set\n",
    "avg_test_loss = test_loss / len(test_set)\n",
    "avg_test_acc = correct_test / len(test_set)\n",
    "\n",
    "print(f\"Test loss: {avg_test_loss:.5f}, Test accuracy: {avg_test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76533a3f-95b0-4a0c-b98a-91062a5a5781",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'Fashionmnist_GNet_Training.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2032a569-c3e5-410e-9111-85647088fd66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
