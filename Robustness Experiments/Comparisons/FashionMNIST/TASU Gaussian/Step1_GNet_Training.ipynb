{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85fdf28c-f372-4dd7-bd54-a7aa2ed65849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import PIL\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "# from Modules import ConvBN, PoolConvBN, PoolLinearBN, SharpCosSim2d, SharpCosSimLinear, LReLU\n",
    "\n",
    "from ConvBN import ConvBN as ConvBN_BiasTrick\n",
    "from LinearBN import LinearBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "748ecd3a-7ffc-4bbb-8694-a567b6e10082",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanH(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.alpha = nn.Parameter(torch.tensor(10.0)) \n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.tanh(self.alpha*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cff8a703-c4d7-4c63-98fc-97d8ef46a5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)) # Normalize with mean 0.5 and std 0.5\n",
    "])\n",
    "\n",
    "batch_size= 1500\n",
    "num_workers=2\n",
    "pin_memory=True\n",
    "\n",
    "dataset = torchvision.datasets.FashionMNIST(root='./Data', train=True, download=True, transform=transform)\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [58000, 2000])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "test_set = torchvision.datasets.FashionMNIST(root='./Data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aced529a-d3ba-4a60-980e-2623e755c88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7565906-2310-4a40-b7a7-627f2e2de45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.conv1_out = 32\n",
    "        self.conv1_size = 5\n",
    "        self.conv1_padding = 2\n",
    "\n",
    "\n",
    "        self.conv2_out = 64\n",
    "        self.conv2_size = 5\n",
    "        self.conv2_padding = 2\n",
    "\n",
    "        self.fc1_out = 512\n",
    "        self.fc2_out = 10\n",
    "\n",
    "        self.q = 1e-6\n",
    "        self.bias_trick_par = nn.Parameter(torch.tensor(0.00005))\n",
    "\n",
    "        # First Convolutional Block\n",
    "\n",
    "        self.block1 = ConvBN_BiasTrick(in_channels=1, out_channels=self.conv1_out, kernel_size=self.conv1_size, padding=self.conv1_padding, std = .05, bias_par_init=0.001)\n",
    "        #self.block2 = ConvBN_BiasTrick(in_channels=self.conv1_out, out_channels=self.conv2_out, kernel_size=self.conv2_size, padding=self.conv2_padding, std = .05, bias_par_init=0.01)\n",
    "\n",
    "        # Second Convolutional Block\n",
    "       \n",
    "        self.block3 = LinearBN(in_features = self.conv1_out * (28//2) * (28//2), out_features=self.fc1_out, std=.3)\n",
    "        \n",
    "        \n",
    "        # torch.manual_seed(0)\n",
    "        self.w2 = nn.Parameter(torch.randn(self.fc1_out, self.fc2_out))\n",
    "        nn.init.normal_(self.w2, mean=0.0, std=.6)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.tanh = TanH()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.max_pool2d(self.tanh(self.block1(x)), (2,2), padding=0)\n",
    "        #x = F.max_pool2d(self.tanh(self.block2(x)), (2,2), padding=0)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.tanh(self.block3(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = x + self.bias_trick_par\n",
    "        x_norm = x / (x.norm(p=2, dim=1, keepdim=True) + self.q)  # Normalize input x\n",
    "        w2_norm = self.w2 / (self.w2.norm(p=2, dim=1, keepdim=True) + self.q)  # Normalize weights\n",
    "        x = torch.matmul(x_norm, w2_norm) # Matrix multiplication \n",
    "\n",
    "        # Return raw logits (no softmax here, CrossEntropyLoss handles it)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fab82256-86aa-432f-a31d-a6f210a08745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] loss: 0.00053 accuracy: 0.7359 val loss: 0.00048 val accuracy: 0.8300 time: 6.44 seconds\n",
      "[epoch 2] loss: 0.00032 accuracy: 0.8505 val loss: 0.00040 val accuracy: 0.8620 time: 6.41 seconds\n",
      "[epoch 3] loss: 0.00028 accuracy: 0.8708 val loss: 0.00041 val accuracy: 0.8555 time: 6.47 seconds\n",
      "[epoch 4] loss: 0.00027 accuracy: 0.8786 val loss: 0.00037 val accuracy: 0.8725 time: 6.60 seconds\n",
      "[epoch 5] loss: 0.00025 accuracy: 0.8889 val loss: 0.00039 val accuracy: 0.8650 time: 6.58 seconds\n",
      "[epoch 6] loss: 0.00024 accuracy: 0.8920 val loss: 0.00035 val accuracy: 0.8810 time: 6.80 seconds\n",
      "[epoch 7] loss: 0.00024 accuracy: 0.8922 val loss: 0.00032 val accuracy: 0.8835 time: 6.55 seconds\n",
      "[epoch 8] loss: 0.00023 accuracy: 0.8981 val loss: 0.00033 val accuracy: 0.8875 time: 6.63 seconds\n",
      "[epoch 9] loss: 0.00023 accuracy: 0.8958 val loss: 0.00033 val accuracy: 0.8805 time: 6.55 seconds\n",
      "[epoch 10] loss: 0.00022 accuracy: 0.8999 val loss: 0.00032 val accuracy: 0.8885 time: 6.65 seconds\n",
      "[epoch 11] loss: 0.00022 accuracy: 0.9042 val loss: 0.00032 val accuracy: 0.8880 time: 6.52 seconds\n",
      "[epoch 12] loss: 0.00022 accuracy: 0.9040 val loss: 0.00030 val accuracy: 0.8965 time: 6.61 seconds\n",
      "[epoch 13] loss: 0.00022 accuracy: 0.9031 val loss: 0.00031 val accuracy: 0.8930 time: 6.56 seconds\n",
      "[epoch 14] loss: 0.00021 accuracy: 0.9103 val loss: 0.00034 val accuracy: 0.8820 time: 6.53 seconds\n",
      "[epoch 15] loss: 0.00021 accuracy: 0.9073 val loss: 0.00031 val accuracy: 0.8895 time: 6.68 seconds\n",
      "[epoch 16] loss: 0.00021 accuracy: 0.9072 val loss: 0.00030 val accuracy: 0.8975 time: 6.62 seconds\n",
      "[epoch 17] loss: 0.00021 accuracy: 0.9090 val loss: 0.00029 val accuracy: 0.8980 time: 6.72 seconds\n",
      "[epoch 18] loss: 0.00020 accuracy: 0.9118 val loss: 0.00031 val accuracy: 0.8900 time: 6.83 seconds\n",
      "[epoch 19] loss: 0.00020 accuracy: 0.9116 val loss: 0.00031 val accuracy: 0.8900 time: 6.60 seconds\n",
      "[epoch 20] loss: 0.00020 accuracy: 0.9136 val loss: 0.00030 val accuracy: 0.8935 time: 6.60 seconds\n",
      "[epoch 21] loss: 0.00020 accuracy: 0.9148 val loss: 0.00029 val accuracy: 0.9035 time: 6.92 seconds\n",
      "[epoch 22] loss: 0.00020 accuracy: 0.9144 val loss: 0.00031 val accuracy: 0.8860 time: 6.58 seconds\n",
      "[epoch 23] loss: 0.00021 accuracy: 0.9091 val loss: 0.00032 val accuracy: 0.8915 time: 6.65 seconds\n",
      "[epoch 24] loss: 0.00020 accuracy: 0.9129 val loss: 0.00030 val accuracy: 0.8970 time: 6.83 seconds\n",
      "[epoch 25] loss: 0.00020 accuracy: 0.9148 val loss: 0.00032 val accuracy: 0.8930 time: 6.56 seconds\n",
      "[epoch 26] loss: 0.00020 accuracy: 0.9143 val loss: 0.00029 val accuracy: 0.9015 time: 6.56 seconds\n",
      "[epoch 27] loss: 0.00019 accuracy: 0.9173 val loss: 0.00030 val accuracy: 0.9005 time: 6.76 seconds\n",
      "[epoch 28] loss: 0.00020 accuracy: 0.9151 val loss: 0.00033 val accuracy: 0.8935 time: 6.62 seconds\n",
      "[epoch 29] loss: 0.00019 accuracy: 0.9184 val loss: 0.00032 val accuracy: 0.8845 time: 6.41 seconds\n",
      "[epoch 30] loss: 0.00020 accuracy: 0.9145 val loss: 0.00031 val accuracy: 0.8965 time: 6.59 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import time  # Import time module\n",
    "\n",
    "train = True\n",
    "model = Network().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Trained 100 Epochs with lr=0.025, 50 epochs with 0.005 and 50 epochs with 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.02, weight_decay=0.00001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "#     model = torch.nn.DataParallel(model)\n",
    "\n",
    "if train:\n",
    "    \n",
    "    loss_hist, acc_hist = [], []\n",
    "    loss_hist_val, acc_hist_val = [], []\n",
    "    \n",
    "    # Initialize variable to track the lowest validation accuracy\n",
    "    best_val_acc = -float('inf')  # Set to negative infinity initially to track the maximum accuracy\n",
    "    \n",
    "    for epoch in range(30):\n",
    "        start_time = time.time()  # Record the start time of the epoch\n",
    "    \n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        for data in train_loader:\n",
    "            batch, labels = data\n",
    "            batch, labels = batch.to(device), labels.to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            # Compute training statistics\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            running_loss += loss.item()\n",
    "    \n",
    "        avg_loss = running_loss / len(train_set)\n",
    "        avg_acc = correct / len(train_set)\n",
    "        loss_hist.append(avg_loss)\n",
    "        acc_hist.append(avg_acc)\n",
    "    \n",
    "        # Validation statistics\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss_val = 0.0\n",
    "            correct_val = 0\n",
    "            for data in val_loader:\n",
    "                batch, labels = data\n",
    "                batch, labels = batch.to(device), labels.to(device)\n",
    "                outputs = model(batch)\n",
    "                loss = criterion(outputs, labels)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "                loss_val += loss.item()\n",
    "            avg_loss_val = loss_val / len(val_set)\n",
    "            avg_acc_val = correct_val / len(val_set)\n",
    "            loss_hist_val.append(avg_loss_val)\n",
    "            acc_hist_val.append(avg_acc_val)\n",
    "        model.train()\n",
    "    \n",
    "        scheduler.step(avg_loss_val)\n",
    "    \n",
    "        # Check if the current validation accuracy is the best we've seen\n",
    "        if avg_acc_val > best_val_acc:\n",
    "            best_val_acc = avg_acc_val\n",
    "            # Save the model with the highest validation accuracy\n",
    "            # if torch.cuda.device_count() > 1:\n",
    "                # torch.save(model.module.state_dict(), 'best_model_mnist.pt')\n",
    "            # else:\n",
    "                # torch.save(model.state_dict(), 'best_model_mnist.pt')\n",
    "    \n",
    "        # Calculate elapsed time\n",
    "        elapsed_time = time.time() - start_time  # Calculate the time taken for this epoch\n",
    "    \n",
    "        print('[epoch %d] loss: %.5f accuracy: %.4f val loss: %.5f val accuracy: %.4f time: %.2f seconds' %\n",
    "              (epoch + 1, avg_loss, avg_acc, avg_loss_val, avg_acc_val, elapsed_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a83be87-bf65-4a82-95b6-89acf3fb356d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.00015, Test accuracy: 0.8966\n"
     ]
    }
   ],
   "source": [
    "# Load the best model saved during training\n",
    "# model.load_state_dict(torch.load('MNIST_GNet_Training_99.33.pth', weights_only=True))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "test_loss = 0.0\n",
    "correct_test = 0\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=2000, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "\n",
    "# Evaluate on the test dataset\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # Get predictions and update the correct count\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "# Compute average loss and accuracy for the test set\n",
    "avg_test_loss = test_loss / len(test_set)\n",
    "avg_test_acc = correct_test / len(test_set)\n",
    "\n",
    "print(f\"Test loss: {avg_test_loss:.5f}, Test accuracy: {avg_test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4543d09-eeda-4d63-bb0b-2ed70bd18758",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'FashionMNIST_GNet_Training_1conv.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa124e7c-9cf9-491e-b2c8-ef752c99c390",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
