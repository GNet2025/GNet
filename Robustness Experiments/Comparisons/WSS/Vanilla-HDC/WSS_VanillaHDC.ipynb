{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d20aaf5-4b1b-4071-8a0b-2aa72e6d3376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from torchvision import datasets, transforms\n",
    "from scipy.io import savemat, loadmat\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "from aeon.datasets import load_classification\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f4cd7c8-5638-4539-95de-0541279dea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset_name = 'WalkingSittingStanding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01554b4a-d8c9-4a2e-b2fb-f9f0cac273d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, metadata = load_classification(dataset_name, return_metadata=True, split='train')\n",
    "X_test, y_test = load_classification(dataset_name, split='test')\n",
    "if X_train.shape[0] < 200:\n",
    "    if X_test.shape[0] >= 200:\n",
    "        train_size = int((X_train.shape[0] + X_test.shape[0]) * 1/4)\n",
    "        x, y = load_classification(dataset_name)\n",
    "        X_train, y_train = x[:train_size, :], y[:train_size]\n",
    "        X_test, y_test = x[train_size:, :], y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7a612a5-9390-477b-a4d7-d55d0e093d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels = 1\n",
    "if X_train.ndim == 3:\n",
    "    input_channels = X_train.shape[1]\n",
    "seq_length = X_train.shape[-1]\n",
    "if y_train.dtype == object or isinstance(y_train[0], str):\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6527de72-9784-4fa0-94a3-46f9258c0f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.reshape(X_train.shape[0], -1))\n",
    "X_test_scaled = scaler.transform(X_test.reshape(X_test.shape[0], -1))\n",
    "\n",
    "\n",
    "X_min = X_train_scaled.min(axis=0)\n",
    "X_max = X_train_scaled.max(axis=0)\n",
    "\n",
    "denom = (X_max - X_min)\n",
    "denom[denom == 0] = 1   # avoid division by zero\n",
    "\n",
    "X_train_norm = (X_train_scaled - X_min) / denom\n",
    "X_test_norm  = (X_test_scaled  - X_min) / denom\n",
    "\n",
    "# Optional: clip to [0,1] just in case\n",
    "X_train_norm = np.clip(X_train_norm, 0, 1)\n",
    "X_test_norm  = np.clip(X_test_norm, 0, 1)\n",
    "X_train_tensor = torch.tensor(X_train_norm, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_norm, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2634afb-9e0e-42bd-8e2f-41137c2a4664",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "num_classes = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa900725-946b-475e-9814-d59361b25c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate bipolar lookup table\n",
    "def lookup_generate(dim: int, datatype: str, n_keys: int, device: torch.device):\n",
    "    if datatype != 'bipolar':\n",
    "        raise ValueError(\"Only 'bipolar' supported\")\n",
    "    tbl = torch.randint(0, 2, (n_keys, dim), device=device, dtype=torch.int8)\n",
    "    return tbl * 2 - 1  # map {0,1} → {-1,+1}\n",
    "\n",
    "# 2. Encode a batch of inputs into hypervectors\n",
    "@torch.no_grad()\n",
    "def encode_batch(X: torch.Tensor, position_table: torch.Tensor, grayscale_table: torch.Tensor):\n",
    "    n_keys = grayscale_table.shape[0]\n",
    "    X_normalized = ((X + 3) / 6) * (n_keys - 1)  # map [-3,3] → [0, n_keys-1]\n",
    "    X_indices = X_normalized.round().clamp(0, n_keys - 1).long()  # indices in [0, n_keys-1]\n",
    "    gray = grayscale_table[X_indices]            # (N, D_in, dim)\n",
    "    pos  = position_table.unsqueeze(0)           # (1, D_in, dim)\n",
    "    hv   = (pos * gray).sum(dim=1)               # (N, dim)\n",
    "    return hv\n",
    "\n",
    "# 3. Train associative memory by summing all encodings per class\n",
    "def train_am(X_train, Y_train, position_table, grayscale_table, dim: int):\n",
    "    H_train = encode_batch(X_train, position_table, grayscale_table).float()  # (N, dim)\n",
    "    C = int(Y_train.max().item()) + 1\n",
    "    am = torch.zeros((C, dim), device=X_train.device, dtype=torch.float32)\n",
    "    am = am.index_add(0, Y_train, H_train)\n",
    "    return am\n",
    "\n",
    "# 4. Single-image prediction (returns class and query HV)\n",
    "@torch.no_grad()\n",
    "def predict_(am, img, position_table, grayscale_table):\n",
    "    qhv = encode_batch(img.unsqueeze(0), position_table, grayscale_table).squeeze(0).float()\n",
    "    sims = F.cosine_similarity(qhv.unsqueeze(0), am, dim=1)  # (C,)\n",
    "    pred = int(sims.argmax().item())\n",
    "    return pred, qhv\n",
    "\n",
    "def predict(am, img, position_table, grayscale_table):\n",
    "    pred, _ = predict_(am, img, position_table, grayscale_table)\n",
    "    return pred\n",
    "\n",
    "# 5. Test on full set\n",
    "@torch.no_grad()\n",
    "def test(am, X_test, Y_test, position_table, grayscale_table):\n",
    "    H_test = encode_batch(X_test, position_table, grayscale_table).float()  # (N_test, dim)\n",
    "    h_norm = H_test.norm(dim=1, keepdim=True)                              # (N,1)\n",
    "    a_norm = am.norm(dim=1, keepdim=True).t()                              # (1,C)\n",
    "    sims   = (H_test @ am.t()) / (h_norm * a_norm)                         # (N,C)\n",
    "    preds  = sims.argmax(dim=1)                                            # (N,)\n",
    "    acc    = (preds == Y_test).float().mean().item()\n",
    "    print(f\"Testing accuracy: {acc:.4f}\")\n",
    "    return acc\n",
    "\n",
    "# 6. Load a saved model (AM + tables)\n",
    "def loadmodel(fpath: str, device: torch.device = None):\n",
    "    with open(fpath, 'rb') as f:\n",
    "        am_np, pos_np, gray_np = pickle.load(f)\n",
    "    am   = torch.from_numpy(am_np)\n",
    "    pos  = torch.from_numpy(pos_np)\n",
    "    gray = torch.from_numpy(gray_np)\n",
    "    if device is not None:\n",
    "        am, pos, gray = am.to(device), pos.to(device), gray.to(device)\n",
    "    return am, pos, gray\n",
    "\n",
    "# 7. Quantize the AM to a lower bit-width\n",
    "def quantize(am: torch.Tensor, before_bw: int, after_bw: int) -> torch.Tensor:\n",
    "    if before_bw <= after_bw:\n",
    "        return am.clone()\n",
    "    shift = before_bw - after_bw\n",
    "    return torch.round(am.float() / (2 ** shift)).to(am.dtype)\n",
    "\n",
    "# 8. Batched AM training\n",
    "@torch.no_grad()\n",
    "def train_am_batched(\n",
    "    X_train: torch.Tensor,\n",
    "    Y_train: torch.Tensor,\n",
    "    position_table: torch.Tensor,\n",
    "    grayscale_table: torch.Tensor,\n",
    "    dim: int,\n",
    "    batch_size: int = 128,\n",
    "    device: torch.device = None\n",
    ") -> torch.Tensor:\n",
    "    N = X_train.shape[0]\n",
    "    C = int(Y_train.max().item()) + 1\n",
    "    am = torch.zeros(C, dim, device=device, dtype=torch.float32)\n",
    "    for i in (range(0, N, batch_size)):\n",
    "        xb = X_train[i : i + batch_size]\n",
    "        yb = torch.as_tensor(Y_train[i : i + batch_size], device=device)  # <== ✅ fixed\n",
    "        hb = encode_batch(xb, position_table, grayscale_table).float()\n",
    "        am = am.index_add(0, yb, hb)\n",
    "    return am\n",
    "\n",
    "# 9. Test on a split (non-batched)\n",
    "@torch.no_grad()\n",
    "def test_split(am, X_split, Y_split, position_table, grayscale_table):\n",
    "    Hs   = encode_batch(X_split, position_table, grayscale_table).float()  # (M, dim)\n",
    "    sims = F.cosine_similarity(Hs.unsqueeze(1), am.unsqueeze(0), dim=2)   # (M, C)\n",
    "    preds = sims.argmax(dim=1)                                            # (M,)\n",
    "    return (preds == Y_split).float().mean().item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def flip_rows_(tensor: torch.Tensor, perc: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    In-place sign flip of `perc` fraction of elements in *every* row.\n",
    "\n",
    "    tensor : (B, D)  – any real dtype (+1/-1 HVs work fine)\n",
    "    perc   : 0‒1     – fraction of dimensions to flip per row\n",
    "\n",
    "    Returns the same tensor object (for chaining).\n",
    "    \"\"\"\n",
    "    if not 0.0 <= perc <= 1.0:\n",
    "        raise ValueError(\"perc must be in [0,1]\")\n",
    "\n",
    "    # trivial cases\n",
    "    if perc == 0.0 or tensor.numel() == 0:\n",
    "        return tensor\n",
    "\n",
    "    B, D = tensor.shape\n",
    "    k = int(round(D * perc))            # exact #dims to flip / row\n",
    "    if k == 0:\n",
    "        return tensor\n",
    "\n",
    "    device = tensor.device\n",
    "    # --- vectorised selection of k unique indices per row -------------\n",
    "    # 1) random scores per entry\n",
    "    rnd = torch.rand(B, D, device=device)\n",
    "    # 2) take indices of the largest k scores along each row\n",
    "    _, idx = rnd.topk(k, dim=1, largest=True, sorted=False)  # (B,k)\n",
    "    # 3) convert (row, col) pairs → flat indices\n",
    "    base = torch.arange(B, device=device).unsqueeze(1) * D   # (B,1)\n",
    "    flat_idx = (idx + base).reshape(-1)                      # (B*k,)\n",
    "    # 4) flip in-place\n",
    "    tensor.view(-1)[flat_idx] *= -1\n",
    "    return tensor\n",
    "    \n",
    "# 11. Test on a split (batched)\n",
    "@torch.no_grad()\n",
    "def test_split_batched(\n",
    "    am: torch.Tensor,\n",
    "    X: torch.LongTensor,\n",
    "    Y: torch.LongTensor,\n",
    "    position_table: torch.Tensor,\n",
    "    grayscale_table: torch.Tensor,\n",
    "    encode_fn,\n",
    "    flip_perc=0.0,\n",
    "    batch_size: int = 128,\n",
    "    device: torch.device = None\n",
    ") -> float:\n",
    "    correct, total = 0, 0\n",
    "    for i in range(0, X.size(0), batch_size):\n",
    "        xb = X[i : i + batch_size].to(device)\n",
    "        yb = Y[i : i + batch_size].to(device)\n",
    "        hb = encode_fn(xb, position_table, grayscale_table).float()\n",
    "        flip_rows_(hb, perc=flip_perc)\n",
    "        sims  = F.cosine_similarity(hb.unsqueeze(1), am.unsqueeze(0), dim=2)\n",
    "        preds = sims.argmax(dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total   += yb.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89259e64-1712-41c6-820a-9717756be4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "296c6b84-bc6a-48d4-a900-f112ac138bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# n_rounds   = 20\n",
    "n_splits = 20\n",
    "split_size = len(test_data) // n_splits\n",
    "flip_percs = np.arange(0.0, 0.51, 0.05)\n",
    "accuracies = np.zeros((len(flip_percs), n_splits))\n",
    "n_class    = num_classes\n",
    "q_bit      = 16\n",
    "print(split_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4aa5e183-a531-4935-a4b6-c229bd2f6672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7352, 3, 206), (2947, 3, 206))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e00421ac-c756-4410-b9e5-2e83525f3192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> Flip Percentage: 0.0\n",
      "Accuracy average for 20 rounds: 69.93197278911565\n",
      "\n",
      "==> Flip Percentage: 0.05\n",
      "Accuracy average for 20 rounds: 69.8639455782313\n",
      "\n",
      "==> Flip Percentage: 0.1\n",
      "Accuracy average for 20 rounds: 68.74149659863946\n",
      "\n",
      "==> Flip Percentage: 0.15000000000000002\n",
      "Accuracy average for 20 rounds: 69.69387755102039\n",
      "\n",
      "==> Flip Percentage: 0.2\n",
      "Accuracy average for 20 rounds: 71.05442176870747\n",
      "\n",
      "==> Flip Percentage: 0.25\n",
      "Accuracy average for 20 rounds: 70.54421768707483\n",
      "\n",
      "==> Flip Percentage: 0.30000000000000004\n",
      "Accuracy average for 20 rounds: 67.51700680272108\n",
      "\n",
      "==> Flip Percentage: 0.35000000000000003\n",
      "Accuracy average for 20 rounds: 68.02721088435374\n",
      "\n",
      "==> Flip Percentage: 0.4\n",
      "Accuracy average for 20 rounds: 67.44897959183672\n",
      "\n",
      "==> Flip Percentage: 0.45\n",
      "Accuracy average for 20 rounds: 63.46938775510205\n",
      "\n",
      "==> Flip Percentage: 0.5\n",
      "Accuracy average for 20 rounds: 19.14965986394558\n"
     ]
    }
   ],
   "source": [
    "for i, perc in enumerate(flip_percs):\n",
    "    print(f\"\\n==> Flip Percentage: {perc}\")\n",
    "    for split_idx in range(n_splits):\n",
    "        # Dynamically get input dimension\n",
    "        input_dim = X_train_tensor.shape[1] \n",
    "        n_keys = 64                   \n",
    "        position_table  = lookup_generate(D, 'bipolar', input_dim, device=device)\n",
    "        grayscale_table = lookup_generate(D, 'bipolar', n_keys, device=device)\n",
    "        am = train_am_batched(\n",
    "            X_train_tensor, y_train_tensor,\n",
    "            position_table, grayscale_table,\n",
    "            dim=D,\n",
    "            batch_size=1,\n",
    "            device=device\n",
    "        )\n",
    "        indices = list(range(len(X_test_tensor)))\n",
    "        np.random.shuffle(indices)  # or random.shuffle(indices)\n",
    "        start = split_idx * split_size\n",
    "        end   = start + split_size\n",
    "        split_indices = indices[start:end]\n",
    "        am_q = quantize(am, before_bw=16, after_bw=q_bit)\n",
    "        acc = test_split_batched(\n",
    "            am_q,\n",
    "            X_test_tensor[split_indices],\n",
    "            y_test_tensor[split_indices],\n",
    "            # X_test_tensor,\n",
    "            # y_test_tensor,\n",
    "            position_table,\n",
    "            grayscale_table,\n",
    "            encode_batch,  \n",
    "            flip_perc=perc,\n",
    "            batch_size=10,\n",
    "            device=device\n",
    "        )\n",
    "        accuracies[i, split_idx] = acc*100\n",
    "\n",
    "    print(\"Accuracy average for 20 rounds:\", accuracies[i].mean())\n",
    "\n",
    "    # Free GPU memory\n",
    "    del position_table, grayscale_table, am, am_q\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84236d48-41f2-4c68-8880-649b2e97e66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([69.93197279, 69.86394558, 68.7414966 , 69.69387755, 71.05442177,\n",
       "       70.54421769, 67.5170068 , 68.02721088, 67.44897959, 63.46938776,\n",
       "       19.14965986])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(accuracies, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a04b8e16-c91a-43b6-b62d-a9ac02676db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "savemat(f'{dataset_name}_VanillaHDC.mat', {f'{dataset_name}_VanillaHDC': accuracies})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcdac05b-eb44-4f73-bb47-87eb805aea1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.67477595, 3.80648567, 3.69153407, 3.23737344, 4.20160969,\n",
       "       4.02511896, 3.29686422, 4.7277688 , 2.36901771, 4.39868385,\n",
       "       2.95174607])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(accuracies, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3772938e-d7a0-4f70-bef0-3cdf39ba3231",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
